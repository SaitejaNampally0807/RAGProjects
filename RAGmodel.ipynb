{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval augmented generation\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# # Load documents from folder\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# # Create vector index\n",
    "# index = VectorStoreIndex.from_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='8fb65928-9f92-4726-bbb0-59b50afa1996', embedding=None, metadata={'page_label': '1', 'file_name': 'Saiteja Nampally AI:ML Engineer.pdf', 'file_path': '/Users/saitejanampally/LLM Projects/RAG/Basic RAG/data/Saiteja Nampally AI:ML Engineer.pdf', 'file_type': 'application/pdf', 'file_size': 9720, 'creation_date': '2025-08-13', 'last_modified_date': '2025-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Saiteja Nampally\\n(201) 492-7499| saitejan.codes@gmail.com\\nProfessional Summary\\nOver 4 years of experience as a Data and AI Engineer developing and deploying AI/ML models using Python, TensorFlow, PyTorch, \\nand scikit-learn. Proficient in designing scalable data pipelines and optimizing cloud solutions on AWS, Azure, and GCP. Demonstrated \\nsuccess in client-facing roles and agile environments, consistently delivering robust, production-ready systems.\\nTechnical Skills\\n• Database Query Languages: T-SQL , PLSQL , NoSQL\\n• Programming Languages : Python, Java, R , C , Node.js\\n• Big Data - Hadoop Ecosystem: Hadoop , Hive, Spark, MapReduce , Pig\\n• Big Data - Spark Technologies: PySpark, Spark SQL, Spark Streaming\\n• ETL Tools: SSIS, Informatica, Glue, GCP dataflow, Apache Spark\\n• Data Visualization & BI: Tableau, Power BI, Domo\\n• Data Warehouse: AWS Redshift, Azure Synapse DB, GCP Big Query, ADF\\n• Non-Relational Databases: MongoDB , Cassandra, DynamoDB\\n• RDBMS : Oracle, MySQL , PostgreSQL, SQL Server\\n• Monitoring & Logging Tools: Stack driver, Prometheus, ELK Stack, Fluentd\\n• Version Control: Git, Bit Bucket, GitLab\\n• Cloud Real Time Processing: GCP , Azure .AWS\\n• Machine Learning: Scikit-learn, TensorFlow, PyTorch, XGBoost , OpenCV\\n• Data Modelling Tools: ER/Studio, Oracle SQL Developer Data Modeler\\n• Professional Skills: Software Development, Agile Development, Client-facing Experience, AI Frameworks, Cloud Technologies\\nProfessional Experience\\nReach Partners Feb 2024\\nData and AI Engineer Austin, TX\\nConsolidated customer and transaction data into PostgreSQL using Python pipelines, enhancing data-driven decision-making for \\neCommerce operations with agile development principles.\\n• Evaluated, selected, and implemented new technologies on GCP Big Query and AWS platforms to optimize data processing and storage \\ncapabilities.\\n• Leveraged AWS services—including S3, Redshift, and Lambda—for parallel data processing and seamless cloud integration.\\n• Designed and deployed machine learning pipelines on AWS SageMaker to predict customer churn with 89% accuracy, applying \\nscikit-learn for model training and optimization.\\n• Orchestrated the migration of a legacy batch processing system to real-time stream processing using Apache Kafka and Spark \\nStreaming, aligning with agile methodologies.\\n• Created interactive data visualization solutions with Python libraries (Matplotlib, Seaborn, Plotly) to communicate insights clearly to \\nstakeholders.\\n• Engineered and maintained scalable data pipelines to collect, process, and store multi-source data, improving accessibility and \\nreliability.\\n• Developed and maintained ETL pipelines for Amazon Redshift, ensuring data integrity and consistency through SSIS-driven processes.\\n• Utilized advanced Python libraries and frameworks including NumPy, pandas, Matplotlib, and scikit-learn for scientific computing and \\ndata analytics in Jupyter Notebook.\\n• Optimized memory management within ETL services to enhance performance and scalability.\\n• Designed efficient data models for complex database systems using ER/Studio, Lucidchart, and Oracle SQL Developer Data Modeler.\\n• Developed and maintained various databases (MySQL, Oracle, PostgreSQL, SQL Server) and crafted SQL queries for effective data \\nextraction.\\nOptum Jan 2023 - Feb 2024\\nCloud Data Engineer Austin, TX\\nConsolidated patient data into PostgreSQL using Python pipelines, supporting data scientists with critical research and clinical trial \\ninsights.\\n• Designed and implemented robust ETL processes using SSIS, GCP Dataflow, Apache Spark, Talend, and Informatica to extract, \\ntransform, and load diverse data sources.\\n• Secured data transfer between on-premises systems and AWS cloud by configuring AWS IAM roles and policies in alignment with \\nbest practices.\\n• Orchestrated complex workflows through scheduled AWS Step Functions to manage batch jobs and data pipelines.\\n• Utilized data warehousing solutions such as GCP Big Query, Oracle Data Warehouse, PostgreSQL, and SQL Server for effective data \\nanalysis and storage.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8762a917-5f6b-492d-9b0a-fe2e3d42da50', embedding=None, metadata={'page_label': '2', 'file_name': 'Saiteja Nampally AI:ML Engineer.pdf', 'file_path': '/Users/saitejanampally/LLM Projects/RAG/Basic RAG/data/Saiteja Nampally AI:ML Engineer.pdf', 'file_type': 'application/pdf', 'file_size': 9720, 'creation_date': '2025-08-13', 'last_modified_date': '2025-08-13'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='• Developed Databricks ETL pipelines using notebooks, Spark DataFrames, Spark SQL, and Python scripting to leverage distributed \\ncomputing capabilities.\\n• Authored scripts to process CSV , JSON, and Parquet files from S3 buckets using Python, enabling seamless data integration into AWS \\nS3, DynamoDB, and Snowflake.\\n• Integrated Apache Spark with Python (PySpark) to process high-volume toll transaction data in parallel, ensuring efficient handling of \\nbig data workloads.\\nWipro Apr 2020 - Aug 2022\\nData Engineer Hyderabad,India\\nConsolidated data into Amazon Redshift using AWS-based Python pipelines and Microsoft SQL Server for interim storage, supporting \\nadvanced analytics and machine learning initiatives.\\n• Engineered Python-based GUI components for front-end functionality and conducted comprehensive testing to ensure software \\nrobustness.\\n• Formulated and executed SQL queries for data validation and analysis on both staging and warehouse tables.\\n• Contributed to dimensional modeling by accurately identifying Facts and Dimensions to streamline data structure for analytical needs.\\n• Performed detailed business requirement analysis, asset management, and gap analysis for source systems to optimize data workflows.\\n• Developed client and internal reports by integrating custom visuals in Power BI, enhancing data interpretation and decision-making.\\n• Applied advanced Tableau techniques—including calculated fields, parameters, and sets—to develop interactive and insightful \\nvisualizations.\\n• Constructed custom visualizations and charts that effectively communicated complex data trends to non-technical audiences.\\n• Authored complex SQL queries, stored procedures, and managed temporary tables to support Power BI and SSRS reporting require-\\nments.\\nEDUCATION\\nUniversity of Bridgeport, CT, USA\\nMasters, Computer Science\\nJBIET, India\\nBachelors, Computer Science', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 2/2 [00:00<00:00, 1583.65it/s]\n",
      "Generating embeddings:   0%|          | 0/2 [00:00<?, ?it/s]2025-08-16 21:08:38,328 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "Generating embeddings: 100%|██████████| 2/2 [00:00<00:00,  2.59it/s]\n"
     ]
    }
   ],
   "source": [
    "index=VectorStoreIndex.from_documents(documents,show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x1027977a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine=index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "retriever = VectorIndexRetriever(index=index, similarity_top_k=4)\n",
    "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.01)\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    node_postprocessors=[postprocessor]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-16 21:08:45,888 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-16 21:08:46,887 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response=query_engine.query(\" does he has python experience?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Yes, he has Python experience.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: 0ad2cfba-a97b-4375-bbcd-91d397ccbe33\n",
      "Similarity: 0.772683644888626\n",
      "Text: Saiteja Nampally (201) 492-7499| saitejan.codes@gmail.com\n",
      "Professional Summary Over 4 years of experience as a Data and AI\n",
      "Engineer developing and deploying AI/ML models using Python,\n",
      "TensorFlow, PyTorch,  and scikit-learn. Proficient in designing\n",
      "scalable data pipelines and optimizing cloud solutions on AWS, Azure,\n",
      "and GCP. Demonstrated  succes...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: 091cded1-2268-4a3e-8317-829cc6f06e45\n",
      "Similarity: 0.7688457501549828\n",
      "Text: • Developed Databricks ETL pipelines using notebooks, Spark\n",
      "DataFrames, Spark SQL, and Python scripting to leverage distributed\n",
      "computing capabilities. • Authored scripts to process CSV , JSON, and\n",
      "Parquet files from S3 buckets using Python, enabling seamless data\n",
      "integration into AWS  S3, DynamoDB, and Snowflake. • Integrated Apache\n",
      "Spark with...\n",
      "Yes, he has Python experience.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "pprint_response(response, show_source=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-16 21:08:51,917 - INFO - Loading all indices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./storage/index_store.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-16 21:08:52,096 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-08-16 21:08:52,560 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saiteja's age is not provided in the context information.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    # 'documents' and 'VectorStoreIndex' are already available in the notebook\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is saiteja age\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
